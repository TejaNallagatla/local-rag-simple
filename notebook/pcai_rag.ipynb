{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCAI PDF RAG System\n",
    "\n",
    "## Complete Implementation of 5 Required Components\n",
    "\n",
    "This notebook demonstrates a working Retrieval-Augmented Generation system using PDF File\n",
    "\n",
    "**Components Implemented:**\n",
    "1. Knowledge Base (PDF)\n",
    "2. Semantic Layer (sentence-transformers)\n",
    "3. Retrieval System (FAISS)\n",
    "4. Augmentation (context enrichment)\n",
    "5. Generation (answer synthesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import All Components\n",
    "\n",
    "Run this cell first to load all modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All components imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import our 5 components\n",
    "from knowledge_base import KnowledgeBase\n",
    "from semantic_layer import SemanticLayer\n",
    "from retrieval_system import RetrievalSystem\n",
    "from augmentation import Augmentation\n",
    "from generation import Generation\n",
    "print(\"âœ… All components imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 1: Knowledge Base\n",
    "\n",
    "Load PCAI dataset from pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Knowledge Base initialized\n",
      "\n",
      "ðŸ“¥ Loading PDF: ../data/hpe-pcai.pdf\n",
      "âœ… Loaded 24 pages from PDF\n",
      "âœ… Created 57 text chunks\n",
      "\n",
      "ðŸ“Š Knowledge Base:\n",
      "   Pages: 24\n",
      "   Chunks: 57\n",
      "   Source: ../data/hpe-pcai.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize KB with PDF and chunk settings\n",
    "kb = KnowledgeBase(pdf_path=\"../data/hpe-pcai.pdf\", chunk_size=200, chunk_overlap=50)\n",
    "\n",
    "# Load PDF pages\n",
    "documents = kb.load_pdf_data()\n",
    "\n",
    "# Create sentence-preserving chunks\n",
    "chunks = kb.create_chunks()\n",
    "\n",
    "# Inspect stats\n",
    "stats = kb.get_stats()\n",
    "stats = kb.get_stats()\n",
    "print(f\"\\nðŸ“Š Knowledge Base:\")\n",
    "print(f\"   Pages: {stats['total_pages']}\")\n",
    "print(f\"   Chunks: {stats['total_chunks']}\")\n",
    "print(f\"   Source: {stats['source']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 2: Semantic Layer\n",
    "\n",
    "Convert text to numerical embeddings for semantic comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  COMPONENT 2: Semantic Layer initializing...\n",
      "   Loading model: all-MiniLM-L6-v2\n",
      "   (First run downloads 22MB - takes 30 seconds)\n",
      "âœ… Model loaded! Embeddings dimension: 384\n",
      "\n",
      "ðŸ”„ Encoding 57 document chunks to embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c07d1a1189c4507ae129d4667fcca9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embeddings created: (57, 384)\n",
      "\n",
      "ðŸ“„ Random preview of 5 chunks with embeddings:\n",
      "\n",
      "--- Sample 1 (Index 30) ---\n",
      "Page: 11\n",
      "Text (first 200 chars): Airflow requirements Power requirements Power requirements Space requirements Space requirements Temperature requirements Temperature requirements Firewall and port requirements Firewall and port requ...\n",
      "Embedding shape: (384,)\n",
      "Embedding sample (first 10 values): [ 0.00880686  0.02316867  0.0089285   0.01078988  0.00056803 -0.03762256\n",
      " -0.00877929  0.03573978 -0.02206737  0.0099078 ]\n",
      "----------------------------------------------------------------------\n",
      "--- Sample 2 (Index 16) ---\n",
      "Page: 7\n",
      "Text (first 200 chars): Essentials Software Documentation AI Essentials Software Documentation . The following sections summarize the key features and capabilities of HPE AI Essentials Software. Core Components Core Componen...\n",
      "Embedding shape: (384,)\n",
      "Embedding sample (first 10 values): [-0.09123894 -0.00673326 -0.02405057  0.04224906  0.03336014 -0.06206925\n",
      "  0.0154468   0.03701079 -0.09258705 -0.0471008 ]\n",
      "----------------------------------------------------------------------\n",
      "--- Sample 3 (Index 26) ---\n",
      "Page: 10\n",
      "Text (first 200 chars): Entity Entity Component Details Component Details Small, Medium and Large Small, Medium and Large Developer System Developer System Software Built on the HPE GreenLake Cloud Platform GreenLake Cloud P...\n",
      "Embedding shape: (384,)\n",
      "Embedding sample (first 10 values): [ 0.00124312 -0.03133019  0.04429704 -0.03265367  0.11083951 -0.0194971\n",
      " -0.03724656  0.05765591  0.00388937  0.04316648]\n",
      "----------------------------------------------------------------------\n",
      "--- Sample 4 (Index 34) ---\n",
      "Page: 13\n",
      "Text (first 200 chars): Networking requirements Networking requirements The following diagrams show networking details for the small, medium, and large configurations of the HPE Private Cloud AI solution. For Developer Syste...\n",
      "Embedding shape: (384,)\n",
      "Embedding sample (first 10 values): [ 0.02376608 -0.07838809  0.07420905 -0.02490907 -0.01905009  0.06559829\n",
      " -0.06980591  0.03591711 -0.06697359 -0.02484385]\n",
      "----------------------------------------------------------------------\n",
      "--- Sample 5 (Index 48) ---\n",
      "Page: 19\n",
      "Text (first 200 chars): TB (adding 1xCNode and 2x JBOFs) 217 TB to 529 TB (adding 1xCNode and 3x JBOFs) 217 TB to 1088 TB (adding 1xCNode and 7x JBOFs) HPE HPE GreenLake for File GreenLake for File Storage (Standard Storage ...\n",
      "Embedding shape: (384,)\n",
      "Embedding sample (first 10 values): [-0.03240145 -0.06121505 -0.06994621 -0.03559827  0.021454    0.00223989\n",
      " -0.00969227  0.07949185 -0.06802074  0.04840973]\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# Random Preview: Chunks & Embeddings\n",
    "# =========================\n",
    "\n",
    "# Number of random samples to preview\n",
    "N = 5\n",
    "\n",
    "# Ensure embeddings exist\n",
    "try:\n",
    "    document_embeddings\n",
    "except NameError:\n",
    "    # Encode chunks if not done yet\n",
    "    semantic = SemanticLayer()\n",
    "    document_embeddings = semantic.encode_documents(kb.chunks)\n",
    "\n",
    "# Random indices\n",
    "random_indices = random.sample(range(len(kb.chunks)), min(N, len(kb.chunks)))\n",
    "\n",
    "print(f\"\\nðŸ“„ Random preview of {N} chunks with embeddings:\\n\")\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    chunk = kb.chunks[idx]\n",
    "    emb = document_embeddings[idx]\n",
    "    \n",
    "    print(f\"--- Sample {i+1} (Index {idx}) ---\")\n",
    "    print(f\"Page: {chunk['page']}\")\n",
    "    print(f\"Text (first 200 chars): {chunk['chunk'][:200]}...\")\n",
    "    print(f\"Embedding shape: {emb.shape}\")\n",
    "    print(f\"Embedding sample (first 10 values): {emb[:10]}\")\n",
    "    print(\"-\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 3: Retrieval System\n",
    "\n",
    "Build FAISS index for fast similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” COMPONENT 3: Retrieval System initialized (dim=384)\n",
      "\n",
      "ðŸ”¨ Building FAISS index...\n",
      "   Documents: 57\n",
      "   Embedding shape: (57, 384)\n",
      "âœ… Index built with 57 vectors\n",
      "\n",
      "âœ… Retrieval system ready to search!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Retrieval System\n",
    "retrieval = RetrievalSystem(dimension=384)\n",
    "\n",
    "# Build searchable index\n",
    "retrieval.build_index(document_embeddings, kb.chunks)\n",
    "\n",
    "print(\"\\nâœ… Retrieval system ready to search!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Searching for top 3 relevant documents...\n",
      "âœ… Retrieved 3 documents\n",
      "Results structure:\n",
      "\n",
      "Result 0:\n",
      "{'rank': 1, 'document': {'page': 1, 'chunk': 'HPE Private Cloud AI 1.5 Administration Guide HPE Private Cloud AI 1.5 Administration Guide Part Number: 20-PCAI-UG-ED1 Published: September 2025 Edition: 1.5'}, 'similarity': 0.4387816354426174, 'distance': 1.2790379524230957}\n",
      "--------------------------------------------------\n",
      "Result 1:\n",
      "{'rank': 2, 'document': {'page': 3, 'chunk': 'Table of contents Table of contents Overview of the HPE Private Cloud AI engineered system Private Cloud AI Stack Developer System HPE AI Essentials: Core Concepts Solution components Optimum environment Airflow requirements Power requirements Space requirements Temperature requirements Firewall and port requirements Networking requirements Licensing requirements Activating the NVIDIA software subscription Solution configurations Configuration sizes Private Cloud AI User roles Signing in to the HPE GreenLake platform and launching Private Cloud AI'}, 'similarity': 0.4371489200265147, 'distance': 1.2875499725341797}\n",
      "--------------------------------------------------\n",
      "Result 2:\n",
      "{'rank': 3, 'document': {'page': 7, 'chunk': 'HPE HPE AI Essentials AI Essentials : Core Concepts : Core Concepts HPE AI Essentials Software is part of the HPE Private Cloud AI software and data foundation layer that accelerates the time from AI pilot to production. HPE AI Essentials Software provides a ready-to-run set of AI and open-source tools to build end-to-end GenAI solutions. From novice to expert, HPE AI Essentials Software provides every user immediate access to a complete suite of proprietary and open-source tools. The no-code/low-code features and automated accelerators empower beginners to quickly build generative AI applications such as chatbots and productivity tools. Experienced developers benefit from APIs, notebooks, and advanced tools that enable rapid experimentation, iteration, and deployment, eliminating IT bottlenecks for resource acquisition and provisioning. NOTE NOTE To start using HPE AI Essentials Software, see the HPE HPE AI Essentials Software Documentation AI Essentials Software Documentation . The following sections summarize the key features and capabilities of HPE AI Essentials Software. Core Components Core Components Open-Source Tools & Frameworks Open-Source Tools & Frameworks Implements a comprehensive managed ecosystem of interconnected open-source tools and frameworks specifically designed for AI workloads.'}, 'similarity': 0.4357204857256046, 'distance': 1.2950493097305298}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Inspect retrieval output\n",
    "results = retrieval.search(semantic.encode_query(\"What are pcai core components\"), top_k=3)\n",
    "print(\"Results structure:\\n\")\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(r)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Complete RAG Pipeline\n",
    "\n",
    "Now let's ask a question and see all 5 components work together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "â“ QUESTION: What are pcai core tools\n",
      "======================================================================\n",
      "\n",
      "[Step 1] Encoding question...\n",
      "[Step 2] Searching knowledge base...\n",
      "\n",
      "ðŸ”Ž Searching for top 3 relevant documents...\n",
      "âœ… Retrieved 3 documents\n",
      "\n",
      "ðŸ“š Retrieved Top 3 Chunks:\n",
      "  â€¢ Page 7 (similarity: 47.9%)\n",
      "    Text snippet: HPE HPE AI Essentials AI Essentials : Core Concepts : Core Concepts HPE AI Essentials Software is part of the HPE Private Cloud AI software and data foundation layer that accelerates the time from AI ...\n",
      "\n",
      "  â€¢ Page 8 (similarity: 47.7%)\n",
      "    Text snippet: AI assistants for the fastest path to inference with unparalleled performance. HPE AI Essentials Software currently includes the following pre-packaged NVIDIA NIM: NVIDIA NIM for GPU accelerated NVIDI...\n",
      "\n",
      "  â€¢ Page 8 (similarity: 47.3%)\n",
      "    Text snippet: NVIDIA Blueprints NVIDIA Blueprints are predefined, customizable AI workflows from NVIDIA that can assist you in creating and deploying generative AI applications. Blueprints are designed for you to d...\n",
      "\n",
      "[Step 3] Creating enriched context...\n",
      "âš¡ COMPONENT 4: Augmentation initialized\n",
      "\n",
      "âš¡ Creating enriched context with 3 documents...\n",
      "âœ… Enriched context created (4297 characters)\n",
      "[Step 4] Generating answer...\n",
      "ðŸ¤– COMPONENT 5: Generation initialized (Ollama: llama3.2:3b)\n",
      "âœ… Ollama connection successful\n",
      "\n",
      "ðŸ¤– Generating answer with LLM...\n",
      "âœ… Answer generated successfully with LLM\n",
      "\n",
      "======================================================================\n",
      "ðŸ’¬ ANSWER:\n",
      "======================================================================\n",
      "Based on the provided context from the PDF, PCAI (not explicitly defined in this text) is not directly mentioned as a core tool. However, there are several AI-related tools and frameworks that are mentioned:\n",
      "\n",
      "1. NVIDIA NIM (Neural Network Inference Module) - a pre-packaged AI framework for GPU accelerated inference.\n",
      "2. NVIDIA Blueprints - predefined, customizable AI workflows from NVIDIA for creating and deploying generative AI applications.\n",
      "\n",
      "Additionally, the text mentions \"Open-Source Tools & Frameworks\" as part of the HPE AI Essentials Software's core components, but it does not specifically mention PCAI as one of them.\n",
      "\n",
      "If we look at the broader context of the topic, PCAI (Plurality Cognitive Architecture) is a type of generative AI model that was introduced by Plurality in 2022. It is designed to generate human-like text and is often used for tasks such as chatbots, content generation, and language translation.\n",
      "\n",
      "While PCAI is not explicitly mentioned in the provided context, it can be inferred that the HPE AI Essentials Software may include tools and frameworks related to PCAI, such as NVIDIA NIM or other open-source AI frameworks. However, without further information, it is difficult to confirm whether PCAI is a core tool within this software.\n",
      "\n",
      "In summary, based on the provided context, there is no clear indication that PCAI is a core tool in HPE AI Essentials Software.\n",
      "\n",
      "======================================================================\n",
      "ðŸ“š SOURCES:\n",
      "======================================================================\n",
      "\n",
      "[1] Page 7 (Similarity: 47.9%)\n",
      "    Preview: HPE HPE AI Essentials AI Essentials : Core Concepts : Core Concepts HPE AI Essentials Software is part of the HPE Private Cloud AI software and data f...\n",
      "\n",
      "[2] Page 8 (Similarity: 47.7%)\n",
      "    Preview: AI assistants for the fastest path to inference with unparalleled performance. HPE AI Essentials Software currently includes the following pre-package...\n",
      "\n",
      "[3] Page 8 (Similarity: 47.3%)\n",
      "    Preview: NVIDIA Blueprints NVIDIA Blueprints are predefined, customizable AI workflows from NVIDIA that can assist you in creating and deploying generative AI ...\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided context from the PDF, PCAI (not explicitly defined in this text) is not directly mentioned as a core tool. However, there are several AI-related tools and frameworks that are mentioned:\\n\\n1. NVIDIA NIM (Neural Network Inference Module) - a pre-packaged AI framework for GPU accelerated inference.\\n2. NVIDIA Blueprints - predefined, customizable AI workflows from NVIDIA for creating and deploying generative AI applications.\\n\\nAdditionally, the text mentions \"Open-Source Tools & Frameworks\" as part of the HPE AI Essentials Software\\'s core components, but it does not specifically mention PCAI as one of them.\\n\\nIf we look at the broader context of the topic, PCAI (Plurality Cognitive Architecture) is a type of generative AI model that was introduced by Plurality in 2022. It is designed to generate human-like text and is often used for tasks such as chatbots, content generation, and language translation.\\n\\nWhile PCAI is not explicitly mentioned in the provided context, it can be inferred that the HPE AI Essentials Software may include tools and frameworks related to PCAI, such as NVIDIA NIM or other open-source AI frameworks. However, without further information, it is difficult to confirm whether PCAI is a core tool within this software.\\n\\nIn summary, based on the provided context, there is no clear indication that PCAI is a core tool in HPE AI Essentials Software.\\n\\n======================================================================\\nðŸ“š SOURCES:\\n======================================================================\\n\\n[1] Page 7 (Similarity: 47.9%)\\n    Preview: HPE HPE AI Essentials AI Essentials : Core Concepts : Core Concepts HPE AI Essentials Software is part of the HPE Private Cloud AI software and data f...\\n\\n[2] Page 8 (Similarity: 47.7%)\\n    Preview: AI assistants for the fastest path to inference with unparalleled performance. HPE AI Essentials Software currently includes the following pre-package...\\n\\n[3] Page 8 (Similarity: 47.3%)\\n    Preview: NVIDIA Blueprints NVIDIA Blueprints are predefined, customizable AI workflows from NVIDIA that can assist you in creating and deploying generative AI ...\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_question(question, top_k=3):\n",
    "    \"\"\"Complete RAG pipeline for a question using PCAI PDF knowledge base\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"â“ QUESTION: {question}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # COMPONENT 2: Encode query\n",
    "    print(\"\\n[Step 1] Encoding question...\")\n",
    "    query_embedding = semantic.encode_query(question)\n",
    "    \n",
    "    # COMPONENT 3: Retrieve relevant chunks\n",
    "    print(\"[Step 2] Searching knowledge base...\")\n",
    "    results = retrieval.search(query_embedding, top_k=top_k)\n",
    "    \n",
    "    print(f\"\\nðŸ“š Retrieved Top {top_k} Chunks:\")\n",
    "    for r in results:\n",
    "        doc_page = r['document']['page']\n",
    "        doc_text = r['document']['chunk']\n",
    "        print(f\"  â€¢ Page {doc_page} (similarity: {r['similarity']:.1%})\")\n",
    "        print(f\"    Text snippet: {doc_text[:200]}...\\n\")\n",
    "    \n",
    "    # COMPONENT 4: Augment with context\n",
    "    print(\"[Step 3] Creating enriched context...\")\n",
    "    augmentor = Augmentation()\n",
    "    enriched = augmentor.create_context(question, results)\n",
    "    \n",
    "    # COMPONENT 5: Generate answer\n",
    "    print(\"[Step 4] Generating answer...\")\n",
    "    generator = Generation(model_name='llama3.2:3b', use_llm=True)\n",
    "    answer = generator.generate(enriched, results)\n",
    "    \n",
    "    # Display answer\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ðŸ’¬ ANSWER:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(answer)\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "ask_question(\"What are pcai core tools\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try More Questions!\n",
    "\n",
    "Test the system with different queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "â“ QUESTION: What is pcai engineered system for?\n",
      "======================================================================\n",
      "\n",
      "[Step 1] Encoding question...\n",
      "[Step 2] Searching knowledge base...\n",
      "\n",
      "ðŸ”Ž Searching for top 3 relevant documents...\n",
      "âœ… Retrieved 3 documents\n",
      "\n",
      "ðŸ“š Retrieved Top 3 Chunks:\n",
      "  â€¢ Page 8 (similarity: 46.9%)\n",
      "    Text snippet: NVIDIA Blueprints NVIDIA Blueprints are predefined, customizable AI workflows from NVIDIA that can assist you in creating and deploying generative AI applications. Blueprints are designed for you to d...\n",
      "\n",
      "  â€¢ Page 1 (similarity: 45.6%)\n",
      "    Text snippet: HPE Private Cloud AI 1.5 Administration Guide HPE Private Cloud AI 1.5 Administration Guide Part Number: 20-PCAI-UG-ED1 Published: September 2025 Edition: 1.5...\n",
      "\n",
      "  â€¢ Page 3 (similarity: 45.4%)\n",
      "    Text snippet: Table of contents Table of contents Overview of the HPE Private Cloud AI engineered system Private Cloud AI Stack Developer System HPE AI Essentials: Core Concepts Solution components Optimum environm...\n",
      "\n",
      "[Step 3] Creating enriched context...\n",
      "âš¡ COMPONENT 4: Augmentation initialized\n",
      "\n",
      "âš¡ Creating enriched context with 3 documents...\n",
      "âœ… Enriched context created (2407 characters)\n",
      "[Step 4] Generating answer...\n",
      "ðŸ¤– COMPONENT 5: Generation initialized (Ollama: llama3.2:3b)\n",
      "âœ… Ollama connection successful\n",
      "\n",
      "ðŸ¤– Generating answer with LLM...\n",
      "âœ… Answer generated successfully with LLM\n",
      "\n",
      "======================================================================\n",
      "ðŸ’¬ ANSWER:\n",
      "======================================================================\n",
      "Based on the provided context, PCAI (Private Cloud AI) is an engineered system designed for use with NVIDIA Blueprints, specifically for creating and deploying generative AI applications, including building AI virtual assistants and digital twins.\n",
      "\n",
      "======================================================================\n",
      "ðŸ“š SOURCES:\n",
      "======================================================================\n",
      "\n",
      "[1] Page 8 (Similarity: 46.9%)\n",
      "    Preview: NVIDIA Blueprints NVIDIA Blueprints are predefined, customizable AI workflows from NVIDIA that can assist you in creating and deploying generative AI ...\n",
      "\n",
      "[2] Page 1 (Similarity: 45.6%)\n",
      "    Preview: HPE Private Cloud AI 1.5 Administration Guide HPE Private Cloud AI 1.5 Administration Guide Part Number: 20-PCAI-UG-ED1 Published: September 2025 Edit...\n",
      "\n",
      "[3] Page 3 (Similarity: 45.4%)\n",
      "    Preview: Table of contents Table of contents Overview of the HPE Private Cloud AI engineered system Private Cloud AI Stack Developer System HPE AI Essentials: ...\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, PCAI (Private Cloud AI) is an engineered system designed for use with NVIDIA Blueprints, specifically for creating and deploying generative AI applications, including building AI virtual assistants and digital twins.\\n\\n======================================================================\\nðŸ“š SOURCES:\\n======================================================================\\n\\n[1] Page 8 (Similarity: 46.9%)\\n    Preview: NVIDIA Blueprints NVIDIA Blueprints are predefined, customizable AI workflows from NVIDIA that can assist you in creating and deploying generative AI ...\\n\\n[2] Page 1 (Similarity: 45.6%)\\n    Preview: HPE Private Cloud AI 1.5 Administration Guide HPE Private Cloud AI 1.5 Administration Guide Part Number: 20-PCAI-UG-ED1 Published: September 2025 Edit...\\n\\n[3] Page 3 (Similarity: 45.4%)\\n    Preview: Table of contents Table of contents Overview of the HPE Private Cloud AI engineered system Private Cloud AI Stack Developer System HPE AI Essentials: ...\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try these questions or write your own!\n",
    "\n",
    "ask_question(\"What is pcai engineered system for?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "â“ QUESTION: is spark used with livy\n",
      "======================================================================\n",
      "\n",
      "[Step 1] Encoding question...\n",
      "[Step 2] Searching knowledge base...\n",
      "\n",
      "ðŸ”Ž Searching for top 3 relevant documents...\n",
      "âœ… Retrieved 3 documents\n",
      "\n",
      "ðŸ“š Retrieved Top 3 Chunks:\n",
      "  â€¢ Page 20 (similarity: 41.1%)\n",
      "    Text snippet: Virtualization Virtualization VM Essentials Data Service Connectors Data Service Connectors HPE -DSC VM with equivalent KickStart Scripts Data Engineering Tools Data Engineering Tools Apache Airflow A...\n",
      "\n",
      "  â€¢ Page 7 (similarity: 41.1%)\n",
      "    Text snippet: Essentials Software Documentation AI Essentials Software Documentation . The following sections summarize the key features and capabilities of HPE AI Essentials Software. Core Components Core Componen...\n",
      "\n",
      "  â€¢ Page 19 (similarity: 38.8%)\n",
      "    Text snippet: support) NIM Mistral 4B Mistral 4B for reranking LORA adapters for model fine-tuning LORA adapters for model fine-tuning Community AI Models and Repositories Aleph Alpha Aleph Alpha , Hugging Face Hug...\n",
      "\n",
      "[Step 3] Creating enriched context...\n",
      "âš¡ COMPONENT 4: Augmentation initialized\n",
      "\n",
      "âš¡ Creating enriched context with 3 documents...\n",
      "âœ… Enriched context created (3541 characters)\n",
      "[Step 4] Generating answer...\n",
      "ðŸ¤– COMPONENT 5: Generation initialized (Ollama: llama3.2:3b)\n",
      "âœ… Ollama connection successful\n",
      "\n",
      "ðŸ¤– Generating answer with LLM...\n",
      "âœ… Answer generated successfully with LLM\n",
      "\n",
      "======================================================================\n",
      "ðŸ’¬ ANSWER:\n",
      "======================================================================\n",
      "Based on the provided context, it appears that Apache Spark is used with Apache Airflow and EzPresto, but not directly with Livy.\n",
      "\n",
      "On Page 7, under \"Data Engineering and Analytics Tools\", HPE AI Essentials Software mentions:\n",
      "\n",
      "* Apache Spark: Enterprise-grade analytics engine supporting large-scale data processing and transformation operations.\n",
      "* Apache Airflow: Advanced workflow orchestration platform that integrates directly with HPE AI Essentials data sets.\n",
      "\n",
      "There is no mention of Livy in this section. However, it's worth noting that Apache Airflow can be used to manage and schedule Spark jobs, including those submitted through Livy.\n",
      "\n",
      "On Page 20, under \"Data Engineering Tools\", HPE AI Essentials Software mentions:\n",
      "\n",
      "* EzPresto: SQL-based query engine optimized for large-scale data analysis.\n",
      "* Superset: Enterprise-class business intelligence platform offering advanced data visualization capabilities.\n",
      "\n",
      "Again, there is no mention of Livy in this section.\n",
      "\n",
      "It's possible that Livy might be used with Apache Spark or another tool in a different configuration, but based on the provided context, it does not appear to be explicitly mentioned as being used with HPE AI Essentials Software.\n",
      "\n",
      "======================================================================\n",
      "ðŸ“š SOURCES:\n",
      "======================================================================\n",
      "\n",
      "[1] Page 20 (Similarity: 41.1%)\n",
      "    Preview: Virtualization Virtualization VM Essentials Data Service Connectors Data Service Connectors HPE -DSC VM with equivalent KickStart Scripts Data Enginee...\n",
      "\n",
      "[2] Page 7 (Similarity: 41.1%)\n",
      "    Preview: Essentials Software Documentation AI Essentials Software Documentation . The following sections summarize the key features and capabilities of HPE AI ...\n",
      "\n",
      "[3] Page 19 (Similarity: 38.8%)\n",
      "    Preview: support) NIM Mistral 4B Mistral 4B for reranking LORA adapters for model fine-tuning LORA adapters for model fine-tuning Community AI Models and Repos...\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, it appears that Apache Spark is used with Apache Airflow and EzPresto, but not directly with Livy.\\n\\nOn Page 7, under \"Data Engineering and Analytics Tools\", HPE AI Essentials Software mentions:\\n\\n* Apache Spark: Enterprise-grade analytics engine supporting large-scale data processing and transformation operations.\\n* Apache Airflow: Advanced workflow orchestration platform that integrates directly with HPE AI Essentials data sets.\\n\\nThere is no mention of Livy in this section. However, it\\'s worth noting that Apache Airflow can be used to manage and schedule Spark jobs, including those submitted through Livy.\\n\\nOn Page 20, under \"Data Engineering Tools\", HPE AI Essentials Software mentions:\\n\\n* EzPresto: SQL-based query engine optimized for large-scale data analysis.\\n* Superset: Enterprise-class business intelligence platform offering advanced data visualization capabilities.\\n\\nAgain, there is no mention of Livy in this section.\\n\\nIt\\'s possible that Livy might be used with Apache Spark or another tool in a different configuration, but based on the provided context, it does not appear to be explicitly mentioned as being used with HPE AI Essentials Software.\\n\\n======================================================================\\nðŸ“š SOURCES:\\n======================================================================\\n\\n[1] Page 20 (Similarity: 41.1%)\\n    Preview: Virtualization Virtualization VM Essentials Data Service Connectors Data Service Connectors HPE -DSC VM with equivalent KickStart Scripts Data Enginee...\\n\\n[2] Page 7 (Similarity: 41.1%)\\n    Preview: Essentials Software Documentation AI Essentials Software Documentation . The following sections summarize the key features and capabilities of HPE AI ...\\n\\n[3] Page 19 (Similarity: 38.8%)\\n    Preview: support) NIM Mistral 4B Mistral 4B for reranking LORA adapters for model fine-tuning LORA adapters for model fine-tuning Community AI Models and Repos...\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"is spark used with livy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "â“ QUESTION: What is the hardware used\n",
      "======================================================================\n",
      "\n",
      "[Step 1] Encoding question...\n",
      "[Step 2] Searching knowledge base...\n",
      "\n",
      "ðŸ”Ž Searching for top 3 relevant documents...\n",
      "âœ… Retrieved 3 documents\n",
      "\n",
      "ðŸ“š Retrieved Top 3 Chunks:\n",
      "  â€¢ Page 8 (similarity: 39.1%)\n",
      "    Text snippet: NVIDIA Blueprints NVIDIA Blueprints are predefined, customizable AI workflows from NVIDIA that can assist you in creating and deploying generative AI applications. Blueprints are designed for you to d...\n",
      "\n",
      "  â€¢ Page 10 (similarity: 38.4%)\n",
      "    Text snippet: Entity Entity Component Details Component Details Small, Medium and Large Small, Medium and Large Developer System Developer System Software Built on the HPE GreenLake Cloud Platform GreenLake Cloud P...\n",
      "\n",
      "  â€¢ Page 8 (similarity: 38.4%)\n",
      "    Text snippet: /mnt/datasources for immediate data access. Implements a shared directory system for team collaboration while maintaining security boundaries. Includes comprehensive tutorials and predictive analysis ...\n",
      "\n",
      "[Step 3] Creating enriched context...\n",
      "âš¡ COMPONENT 4: Augmentation initialized\n",
      "\n",
      "âš¡ Creating enriched context with 3 documents...\n",
      "âœ… Enriched context created (4337 characters)\n",
      "[Step 4] Generating answer...\n",
      "ðŸ¤– COMPONENT 5: Generation initialized (Ollama: llama3.2:3b)\n",
      "âœ… Ollama connection successful\n",
      "\n",
      "ðŸ¤– Generating answer with LLM...\n",
      "âœ… Answer generated successfully with LLM\n",
      "\n",
      "======================================================================\n",
      "ðŸ’¬ ANSWER:\n",
      "======================================================================\n",
      "The hardware used in the NVIDIA Blueprints and HPE AI Essentials environment is:\n",
      "\n",
      "1. NVIDIA GPUs (Graphics Processing Units)\n",
      "2. HPE GreenLake for File Storage with a minimum capacity of at least 109 TB\n",
      "3. Local NFS with 32TB of internal file and object storage.\n",
      "\n",
      "Note that the specific hardware specifications are not provided in the context, but these are the components mentioned as being used in the environment.\n",
      "\n",
      "======================================================================\n",
      "ðŸ“š SOURCES:\n",
      "======================================================================\n",
      "\n",
      "[1] Page 8 (Similarity: 39.1%)\n",
      "    Preview: NVIDIA Blueprints NVIDIA Blueprints are predefined, customizable AI workflows from NVIDIA that can assist you in creating and deploying generative AI ...\n",
      "\n",
      "[2] Page 10 (Similarity: 38.4%)\n",
      "    Preview: Entity Entity Component Details Component Details Small, Medium and Large Small, Medium and Large Developer System Developer System Software Built on ...\n",
      "\n",
      "[3] Page 8 (Similarity: 38.4%)\n",
      "    Preview: /mnt/datasources for immediate data access. Implements a shared directory system for team collaboration while maintaining security boundaries. Include...\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The hardware used in the NVIDIA Blueprints and HPE AI Essentials environment is:\\n\\n1. NVIDIA GPUs (Graphics Processing Units)\\n2. HPE GreenLake for File Storage with a minimum capacity of at least 109 TB\\n3. Local NFS with 32TB of internal file and object storage.\\n\\nNote that the specific hardware specifications are not provided in the context, but these are the components mentioned as being used in the environment.\\n\\n======================================================================\\nðŸ“š SOURCES:\\n======================================================================\\n\\n[1] Page 8 (Similarity: 39.1%)\\n    Preview: NVIDIA Blueprints NVIDIA Blueprints are predefined, customizable AI workflows from NVIDIA that can assist you in creating and deploying generative AI ...\\n\\n[2] Page 10 (Similarity: 38.4%)\\n    Preview: Entity Entity Component Details Component Details Small, Medium and Large Small, Medium and Large Developer System Developer System Software Built on ...\\n\\n[3] Page 8 (Similarity: 38.4%)\\n    Preview: /mnt/datasources for immediate data access. Implements a shared directory system for team collaboration while maintaining security boundaries. Include...\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your own question here!\n",
    "ask_question(\"What is the hardware used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Mode\n",
    "\n",
    "Keep asking questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question (or 'quit' to stop):  what are solution components,  how are they used\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "â“ QUESTION: what are solution components,  how are they used\n",
      "======================================================================\n",
      "\n",
      "[Step 1] Encoding question...\n",
      "[Step 2] Searching knowledge base...\n",
      "\n",
      "ðŸ”Ž Searching for top 3 relevant documents...\n",
      "âœ… Retrieved 3 documents\n",
      "\n",
      "ðŸ“š Retrieved Top 3 Chunks:\n",
      "  â€¢ Page 9 (similarity: 43.5%)\n",
      "    Text snippet: and permission management Features sophisticated data source connection management with security controls Allows administrators to configure and customize included applications through the user interf...\n",
      "\n",
      "  â€¢ Page 10 (similarity: 42.5%)\n",
      "    Text snippet: Entity Entity Component Details Component Details Small, Medium and Large Small, Medium and Large Developer System Developer System Software Built on the HPE GreenLake Cloud Platform GreenLake Cloud P...\n",
      "\n",
      "  â€¢ Page 8 (similarity: 40.6%)\n",
      "    Text snippet: NVIDIA Blueprints NVIDIA Blueprints are predefined, customizable AI workflows from NVIDIA that can assist you in creating and deploying generative AI applications. Blueprints are designed for you to d...\n",
      "\n",
      "[Step 3] Creating enriched context...\n",
      "âš¡ COMPONENT 4: Augmentation initialized\n",
      "\n",
      "âš¡ Creating enriched context with 3 documents...\n",
      "âœ… Enriched context created (3455 characters)\n",
      "[Step 4] Generating answer...\n",
      "ðŸ¤– COMPONENT 5: Generation initialized (Ollama: llama3.2:3b)\n",
      "âœ… Ollama connection successful\n",
      "\n",
      "ðŸ¤– Generating answer with LLM...\n",
      "âœ… Answer generated successfully with LLM\n",
      "\n",
      "======================================================================\n",
      "ðŸ’¬ ANSWER:\n",
      "======================================================================\n",
      "Based on the provided context from Page 9 of the PDF:\n",
      "\n",
      "Solution components are a set of predefined, customizable AI workflows (Blueprints) and software components that are part of HPE Private Cloud AI.\n",
      "\n",
      "These solution components include:\n",
      "\n",
      "* Developer System\n",
      "* Virtualization\n",
      "* Data Service Connectors\n",
      "* Storage (HPE GreenLake for File Storage)\n",
      "\n",
      "They are used to provide a robust infrastructure and customization capabilities to businesses, allowing them to transform their unique expertise and data into groundbreaking AI solutions that drive competitive advantage and unlock new opportunities in their respective fields.\n",
      "\n",
      "In other words, solution components are the building blocks of HPE Private Cloud AI, providing a comprehensive set of tools and technologies for creating and deploying generative AI applications.\n",
      "\n",
      "======================================================================\n",
      "ðŸ“š SOURCES:\n",
      "======================================================================\n",
      "\n",
      "[1] Page 9 (Similarity: 43.5%)\n",
      "    Preview: and permission management Features sophisticated data source connection management with security controls Allows administrators to configure and custo...\n",
      "\n",
      "[2] Page 10 (Similarity: 42.5%)\n",
      "    Preview: Entity Entity Component Details Component Details Small, Medium and Large Small, Medium and Large Developer System Developer System Software Built on ...\n",
      "\n",
      "[3] Page 8 (Similarity: 40.6%)\n",
      "    Preview: NVIDIA Blueprints NVIDIA Blueprints are predefined, customizable AI workflows from NVIDIA that can assist you in creating and deploying generative AI ...\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Interactive questioning\n",
    "while True:\n",
    "    user_question = input(\"\\nYour question (or 'quit' to stop): \").strip()\n",
    "    \n",
    "    if user_question.lower() in ['quit', 'exit', 'q']:\n",
    "        print(\"ðŸ‘‹ PDF RAG here to answer\")\n",
    "        break\n",
    "    \n",
    "    if user_question:\n",
    "        ask_question(user_question)\n",
    "    else:\n",
    "        print(\"Please enter a question!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
